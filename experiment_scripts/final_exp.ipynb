{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from datapath_manager import DataPathManager, ITWDataPathManager\n",
    "from trainers import MachineLearningModelTrainer, BranchNeuralNetworkTrainer\n",
    "from dataloader import EmbeddingDataLoader\n",
    "from evaluators import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 'ltkhiem'\n",
    "dates = ['2022-10-16', '2022-10-17', '2022-10-18', '2022-10-20', '2022-10-21', '2022-10-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.abspath(f'./models/{user_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-based model for the targeted user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_dataset_name = 'DCU_NVT_EXP2'\n",
    "window_size = 60\n",
    "window_shift = 0.25\n",
    "signal_type = 'bvp_eda_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_manager = DataPathManager(lab_dataset_name)\n",
    "lab_feature_folder_path = os.path.dirname(dp_manager.get_feature_path(user_id, signal_type, window_size, window_shift))\n",
    "ground_truth_path = os.path.join(lab_feature_folder_path, 'ground_truth.npy')\n",
    "tasks_index_path = os.path.join(lab_feature_folder_path, 'tasks_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_features = []\n",
    "signals = ['bvp', 'eda', 'temp']\n",
    "for signal in signals:\n",
    "    signal_path = os.path.join(lab_feature_folder_path, f'{signal}.npy')\n",
    "    signal_data = np.load(signal_path)\n",
    "    lab_features.append(signal_data)\n",
    "lab_features = np.concatenate(lab_features, axis=1)\n",
    "tasks_index = np.load(tasks_index_path)\n",
    "ground_truth = np.load(ground_truth_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test lab-based model\n",
    "def split_train_test(indices, test_size: float = 0.3):\n",
    "        \"\"\"\n",
    "        Split train and test data for subject-dependent model training:\n",
    "            - Train_data: (1 - test_size) * number of data of a class\n",
    "            - Test_data: test_size * number of data of a class \n",
    "        NOTE: This means that this approach of data splitting simulate the real-life situation \n",
    "        where the test data is the segment of data that is recorded later after we have the train data.\n",
    "        \"\"\"\n",
    "        cut_point = int((1 - test_size) * len(indices))\n",
    "        train_indices = indices[:cut_point].tolist()\n",
    "        test_indices = indices[cut_point:].tolist()\n",
    "        return train_indices, test_indices\n",
    "\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "VALID_SIZE = 0.1\n",
    "indices = np.arange(lab_features.shape[0]) # The indices of the lab-based features\n",
    "train_indices, valid_indices, test_indices = [], [], []\n",
    "for _, task_test_index in LeaveOneGroupOut().split(indices, y=None, groups=tasks_index):\n",
    "    task_train_indices, task_test_indices = split_train_test(indices[task_test_index], test_size = TEST_SIZE)\n",
    "    task_train_indices, task_valid_indices = split_train_test(indices[task_train_indices], test_size = VALID_SIZE)\n",
    "    train_indices += task_train_indices\n",
    "    valid_indices += task_valid_indices\n",
    "    test_indices += task_test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep-Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = lab_features[train_indices], ground_truth[train_indices]\n",
    "X_valid, y_valid = lab_features[valid_indices], ground_truth[valid_indices]\n",
    "X_test, y_test = lab_features[test_indices], ground_truth[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = EmbeddingDataLoader(X_train, y_train)\n",
    "validate_dataloader = EmbeddingDataLoader(X_valid, y_valid)\n",
    "test_dataloader = EmbeddingDataLoader(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load deep model configuration\n",
    "user_model_saved_path = os.path.join(f'{model_path}/lab_deep_fusion.pth')\n",
    "config_path = os.path.join(\n",
    "    os.path.dirname(os.getcwd()), \n",
    "    'models', 'model_config', \n",
    "    f'branchnn_sensor_combination_{signal_type}.yaml'\n",
    ")\n",
    "config_dict = yaml.safe_load(open(config_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = BranchNeuralNetworkTrainer('./logs.txt', \n",
    "    user_model_saved_path, \n",
    "    config_dict, \n",
    "    target_metrics=['balanced_accuracy', 'f1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clf.train(train_dataloader, validate_dataloader, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_clf.predict(test_dataloader)\n",
    "print(Evaluator().evaluate(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ExtraTreesClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([lab_features[train_indices], lab_features[valid_indices]], axis=0)\n",
    "y_train = np.concatenate([ground_truth[train_indices], ground_truth[valid_indices]], axis=0)\n",
    "X_test, y_test = lab_features[test_indices], ground_truth[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators = 500,\n",
    "    random_state = 0, \n",
    "    n_jobs = -1, \n",
    "    max_features = 'sqrt', \n",
    "    max_depth = 8, \n",
    "    min_samples_split = 2, \n",
    "    min_samples_leaf = 8,\n",
    "    oob_score = True, \n",
    "    bootstrap = True, \n",
    "    class_weight = 'balanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = et_clf.predict(X_test)\n",
    "print(Evaluator().evaluate(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'model': et_clf, 'scaler': None}\n",
    "joblib.dump(data, os.path.join(model_path, 'lab_et_clf.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train = std_scaler.fit_transform(np.concatenate([lab_features[train_indices], lab_features[valid_indices]], axis=0))\n",
    "y_train = np.concatenate([ground_truth[train_indices], ground_truth[valid_indices]], axis=0)\n",
    "X_test, y_test = std_scaler.transform(lab_features[test_indices]), ground_truth[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(\n",
    "    random_state = 0,\n",
    "    class_weight = 'balanced',\n",
    "    n_jobs = -1,\n",
    "    solver = 'saga',\n",
    "    max_iter = 50000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_classifier.predict(X_test)\n",
    "print(Evaluator().evaluate(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'model': lr_classifier, 'scaler': std_scaler}\n",
    "joblib.dump(data, os.path.join(model_path, 'lab_lr_clf.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-the-wild model for the targeted user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_dataset_name = 'DCU_EXP2_ITW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(user_id: str, date: str,):\n",
    "    dataset_path = ITWDataPathManager(itw_dataset_name).get_dataset_path()\n",
    "\n",
    "    user_date_feature_path = os.path.join(dataset_path, 'features', user_id, date)\n",
    "    feature_path = os.path.join(user_date_feature_path, 'X.npy')\n",
    "    gt_path = os.path.join(user_date_feature_path, 'y.npy')\n",
    "\n",
    "    feat = np.nan_to_num(np.load(feature_path))[:, :72]\n",
    "    gt = np.load(gt_path)\n",
    "    return feat, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "feat_dates = [get_features_and_labels(user_id, date) for date in dates]\n",
    "for x in feat_dates:\n",
    "    print(Counter(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply lab-based model to in-the-wild data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_itw = np.concatenate([x[0] for x in feat_dates], axis=0)\n",
    "y_test_itw = np.concatenate([x[1] for x in feat_dates], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_test_dataloader = EmbeddingDataLoader(X_test_itw, y_test_itw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_user_model_saved_path = os.path.abspath(f'{model_path}/itw_deep_fusion.pth')\n",
    "itw_df_clf = BranchNeuralNetworkTrainer('./logs.txt',\n",
    "    itw_user_model_saved_path,\n",
    "    config_dict,\n",
    "    target_metrics=['balanced_accuracy', 'f1'],\n",
    "    pretrained_model_path = user_model_saved_path\n",
    ")\n",
    "itw_et_clf = joblib.load(os.path.join(model_path, 'lab_et_clf.pkl'))['model']\n",
    "itw_lr_clf = joblib.load(os.path.join(model_path, 'lab_lr_clf.pkl'))['model']\n",
    "std_scaler = joblib.load(os.path.join(model_path, 'lab_lr_clf.pkl'))['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab-based Deep Fusion model applied to ITW data\n",
    "print(\"--- Deep Fusion ---\")\n",
    "y_pred_itw = itw_df_clf.predict(itw_test_dataloader)\n",
    "print(Evaluator().evaluate(y_test_itw, y_pred_itw))\n",
    "print(\"--- Extra Trees ---\")\n",
    "y_pred_itw = itw_et_clf.predict(X_test_itw)\n",
    "print(Evaluator().evaluate(y_test_itw, y_pred_itw))\n",
    "print(\"--- Logistic Regression ---\")\n",
    "y_pred_itw = itw_lr_clf.predict(std_scaler.transform(X_test_itw))\n",
    "print(Evaluator().evaluate(y_test_itw, y_pred_itw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-tune the lab-based model to adapt to the in-the-wild data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 3 days for training\n",
    "X_train = np.concatenate([x[0] for x in feat_dates[:3]])\n",
    "y_train = np.concatenate([x[1] for x in feat_dates[:3]]).astype(int)\n",
    "# Use the last 3 days for testing\n",
    "X_test = np.concatenate([x[0] for x in feat_dates[3:]])\n",
    "y_test = np.concatenate([x[1] for x in feat_dates[3:]]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Deep-Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_train_dataloader = EmbeddingDataLoader(X_train, y_train)\n",
    "itw_test_dataloader = EmbeddingDataLoader(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_df_clf.train(itw_train_dataloader, itw_test_dataloader, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_itw = itw_df_clf.predict(itw_test_dataloader)\n",
    "print(Evaluator().evaluate(y_test, y_pred_itw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Re-train ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train ExtraTreesClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_et_clf = ExtraTreesClassifier(\n",
    "    n_estimators = 500,\n",
    "    random_state = 0, \n",
    "    n_jobs = -1, \n",
    "    max_features = 'sqrt', \n",
    "    max_depth = 8, \n",
    "    min_samples_split = 0.005, \n",
    "    min_samples_leaf = 0.005,\n",
    "    oob_score = True, \n",
    "    bootstrap = True, \n",
    "    class_weight = 'balanced'\n",
    ")\n",
    "itw_et_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_itw = itw_et_clf.predict(X_test)\n",
    "print(Evaluator().evaluate(y_test, y_pred_itw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_lr_clf = LogisticRegression(\n",
    "    random_state = 0,\n",
    "    class_weight = 'balanced',\n",
    "    n_jobs = -1,\n",
    "    solver = 'saga',\n",
    "    max_iter = 10000,\n",
    ")\n",
    "itw_lr_clf.fit(scaler.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_itw = itw_lr_clf.predict(scaler.transform(X_test))\n",
    "print(Evaluator().evaluate(y_test, y_pred_itw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('stress')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ebc3d14ab14189c4eae9bca0430f788ed6f529827f083fec73cc217c56eda2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
